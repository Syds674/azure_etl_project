
# ðŸ“Š Projeto de ETL com Databricks e Azure via Kaggle

Este projeto tem como objetivo realizar um pipeline de ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga) em uma base de dados obtida via Kaggle. AtravÃ©s da plataforma **Databricks Community Edition**, os dados sÃ£o baixados e processados com dois notebooks principais: `data_ingestion` e `data_processing`.

## ðŸ§© VisÃ£o Geral

- **ExtraÃ§Ã£o**: Os dados sÃ£o obtidos diretamente do Kaggle (via API), armazenados no Databricks DBFS e depois organizados em diretÃ³rios estruturados.
- **TransformaÃ§Ã£o**: Aplicamos limpeza e tratamento dos dados para uso posterior.
- **Carga**: Os dados processados podem ser salvos no Azure Data Lake, SQL ou outro destino, conforme a extensÃ£o do projeto.

## ðŸ“ Notebooks Utilizados

| Notebook         | FunÃ§Ã£o                                                                 |
|------------------|------------------------------------------------------------------------|
| `data_ingestion` | ResponsÃ¡vel por baixar os dados do Kaggle e armazenÃ¡-los no DBFS       |
| `data_processing`| Realiza o processamento, limpeza e transformaÃ§Ã£o dos dados extraÃ­dos   |

---

## ðŸš€ Como Usar Este Projeto

### 1. Clone este repositÃ³rio
```bash
git clone https://github.com/seu-usuario/seu-repo.git
cd seu-repo
```

### 2. FaÃ§a login no [Databricks Community Edition](https://community.cloud.databricks.com/)

Crie uma conta gratuita se ainda nÃ£o tiver.

---

### 3. Importe os notebooks

#### OpÃ§Ã£o A: Upload manual
1. No Databricks, vÃ¡ atÃ© o menu lateral esquerdo â†’ Workspace
2. Crie uma pasta com o nome do projeto
3. Clique com o botÃ£o direito na pasta â†’ Import
4. Importe os arquivos `data_ingestion.py` e `data_processing.py` (ou `.ipynb`)

#### OpÃ§Ã£o B: Reimportar via `.dbc` (se disponÃ­vel)
1. VÃ¡ em Workspace â†’ Import
2. Importe o arquivo `.dbc` (Databricks Archive) diretamente

---

### 4. Configure sua conta Kaggle (API)
1. Acesse seu perfil no [Kaggle](https://www.kaggle.com/)
2. VÃ¡ em "Account" > "Create API Token"
3. Baixe o arquivo `kaggle.json`
4. No Databricks:
   - VÃ¡ em `Data` â†’ `DBFS` â†’ `/FileStore`
   - FaÃ§a o upload do arquivo `kaggle.json`
   - Configure no notebook o caminho: `/dbfs/FileStore/kaggle.json`

---

### 5. Execute os notebooks
1. Execute o `data_ingestion` primeiro
2. Depois execute o `data_processing`

---

## ðŸ“Œ Requisitos

- Conta no [Kaggle](https://www.kaggle.com/)
- Conta no [Databricks Community Edition](https://community.cloud.databricks.com/)
- PermissÃ£o para upload no DBFS
- (Opcional) Conta no Azure para armazenar os dados processados

---

## ðŸ“¬ Contato

Se tiver dÃºvidas ou quiser contribuir, fique Ã  vontade para abrir uma *issue* ou *pull request*!
