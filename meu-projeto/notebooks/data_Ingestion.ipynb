{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0df8097-5d25-4e40-90cc-32b86b8f9b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting kaggle\n  Downloading kaggle-1.7.4.5-py3-none-any.whl (181 kB)\nRequirement already satisfied: charset-normalizer in /databricks/python3/lib/python3.9/site-packages (from kaggle) (2.0.4)\nCollecting python-slugify\n  Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\nCollecting tqdm\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from kaggle) (2.27.1)\nRequirement already satisfied: protobuf in /databricks/python3/lib/python3.9/site-packages (from kaggle) (3.19.4)\nRequirement already satisfied: python-dateutil>=2.5.3 in /databricks/python3/lib/python3.9/site-packages (from kaggle) (2.8.2)\nRequirement already satisfied: idna in /databricks/python3/lib/python3.9/site-packages (from kaggle) (3.3)\nCollecting text-unidecode\n  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\nRequirement already satisfied: six>=1.10 in /databricks/python3/lib/python3.9/site-packages (from kaggle) (1.16.0)\nRequirement already satisfied: urllib3>=1.15.1 in /databricks/python3/lib/python3.9/site-packages (from kaggle) (1.26.9)\nRequirement already satisfied: setuptools>=21.0.0 in /databricks/python3/lib/python3.9/site-packages (from kaggle) (61.2.0)\nRequirement already satisfied: webencodings in /databricks/python3/lib/python3.9/site-packages (from kaggle) (0.5.1)\nRequirement already satisfied: certifi>=14.05.14 in /databricks/python3/lib/python3.9/site-packages (from kaggle) (2021.10.8)\nRequirement already satisfied: bleach in /databricks/python3/lib/python3.9/site-packages (from kaggle) (4.1.0)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from bleach->kaggle) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->bleach->kaggle) (3.0.4)\nInstalling collected packages: text-unidecode, tqdm, python-slugify, kaggle\nSuccessfully installed kaggle-1.7.4.5 python-slugify-8.0.4 text-unidecode-1.3 tqdm-4.67.1\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting kagglehub\n  Downloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\nCollecting pyyaml\n  Downloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from kagglehub) (21.3)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-4389df0b-6717-41b1-adcf-b4e87823ba07/lib/python3.9/site-packages (from kagglehub) (4.67.1)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from kagglehub) (2.27.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->kagglehub) (3.0.4)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests->kagglehub) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests->kagglehub) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests->kagglehub) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests->kagglehub) (1.26.9)\nInstalling collected packages: pyyaml, kagglehub\nSuccessfully installed kagglehub-0.3.12 pyyaml-6.0.2\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "# 1) Instalação da dependência Kaggle\n",
    "%pip install kaggle\n",
    "%pip install kagglehub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b9ff032-18ea-4c16-b320-e98319f6900e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40e5018-3760-433b-a64a-f14598a9f6bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Configurar ambiente Kaggle a partir do arquivo no DBFS\n",
    "def setup_kaggle_credentials():\n",
    "    try:\n",
    "        # Leitura do arquivo usando DBFS path\n",
    "        kaggle_json_path = \"/dbfs/FileStore/tables/kaggle.json\"\n",
    "        \n",
    "        # Verificar se o arquivo existe\n",
    "        if not os.path.exists(kaggle_json_path):\n",
    "            print(f\"Arquivo não encontrado: {kaggle_json_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Carregar credenciais do arquivo\n",
    "        with open(kaggle_json_path) as f:\n",
    "            creds = json.load(f)\n",
    "        \n",
    "        # Configurar variáveis de ambiente\n",
    "        os.environ['KAGGLE_USERNAME'] = creds['username']\n",
    "        os.environ['KAGGLE_KEY'] = creds['key']\n",
    "        \n",
    "        print(\"Credenciais do Kaggle configuradas com sucesso!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao configurar credenciais: {str(e)}\")\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e8a2a1-fd93-4d6e-ae1f-0c9c12ad884b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Configuração inicial do Spark\n",
    "spark = SparkSession.builder.appName(\"KaggleToDBFS\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e7c3a8-a167-4a94-9479-8d23ba1f1622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório já existe: /FileStore/tables/data_ingestion\n"
     ]
    }
   ],
   "source": [
    "# 3. Verificar/Criar pasta no DBFS\n",
    "def ensure_directory_exists(path):\n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "    dbfs_path = spark._jvm.org.apache.hadoop.fs.Path(path)\n",
    "    if not fs.exists(dbfs_path):\n",
    "        print(f\"Criando diretório: {path}\")\n",
    "        fs.mkdirs(dbfs_path)\n",
    "    else:\n",
    "        print(f\"Diretório já existe: {path}\")\n",
    "\n",
    "dbfs_dir = \"/FileStore/tables/data_ingestion\"\n",
    "ensure_directory_exists(dbfs_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5403cc57-ab02-428d-bb8f-7c1e58b71958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nConfigurando credenciais Kaggle...\nArquivo não encontrado: /dbfs/FileStore/tables/kaggle.json\nTentando método alternativo de leitura...\nCredenciais configuradas com sucesso via dbutils!\n"
     ]
    }
   ],
   "source": [
    "# 4. Configurar credenciais Kaggle\n",
    "print(\"\\nConfigurando credenciais Kaggle...\")\n",
    "if not setup_kaggle_credentials():\n",
    "    # Tentar método alternativo usando dbutils\n",
    "    print(\"Tentando método alternativo de leitura...\")\n",
    "    try:\n",
    "        # Ler o arquivo diretamente do DBFS usando dbutils\n",
    "        kaggle_json_content = dbutils.fs.head(\"/FileStore/tables/kaggle.json\", 10000)\n",
    "        creds = json.loads(kaggle_json_content)\n",
    "        \n",
    "        os.environ['KAGGLE_USERNAME'] = creds['username']\n",
    "        os.environ['KAGGLE_KEY'] = creds['key']\n",
    "        \n",
    "        print(\"Credenciais configuradas com sucesso via dbutils!\")\n",
    "    except Exception as alt_e:\n",
    "        print(f\"Falha no método alternativo: {str(alt_e)}\")\n",
    "        dbutils.notebook.exit(\"Falha na configuração das credenciais. Abortando.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13da267b-5a8c-49d8-ae7c-a188aa272a38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n==================================================\nIniciando download do dataset...\nDownloading from https://www.kaggle.com/api/v1/datasets/download/karkavelrajaj/amazon-sales-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0.00/1.95M [00:00<?, ?B/s]\r100%|██████████| 1.95M/1.95M [00:00<00:00, 27.2MB/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\nArquivo baixado em: /root/.cache/kagglehub/datasets/karkavelrajaj/amazon-sales-dataset/versions/1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Download do dataset\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Iniciando download do dataset...\")\n",
    "try:\n",
    "    download_path = kagglehub.dataset_download(\"karkavelrajaj/amazon-sales-dataset\")\n",
    "    print(f\"Arquivo baixado em: {download_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro no download: {str(e)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56cddcba-b5e5-4823-94bf-0af7b2db8f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nVerificando arquivos existentes do mesmo dia...\n"
     ]
    }
   ],
   "source": [
    "# 6. Função para remover arquivos existentes do mesmo dia\n",
    "def remove_existing_files_for_today():\n",
    "    # Formato de data para comparação (AAAAMMDD)\n",
    "    today_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # Listar arquivos no diretório\n",
    "    file_list = dbutils.fs.ls(dbfs_dir)\n",
    "    \n",
    "    # Filtrar arquivos que começam com a data de hoje\n",
    "    for file_info in file_list:\n",
    "        file_name = os.path.basename(file_info.path)\n",
    "        if file_name.startswith(today_str + \"_\"):\n",
    "            print(f\"Removendo arquivo existente do mesmo dia: {file_info.path}\")\n",
    "            dbutils.fs.rm(file_info.path)\n",
    "\n",
    "# 7. Remover arquivos existentes do mesmo dia antes de copiar\n",
    "print(\"\\nVerificando arquivos existentes do mesmo dia...\")\n",
    "remove_existing_files_for_today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6ef0d2e-09cf-4f7b-93f5-ca224924ba17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n==================================================\nCopiando arquivos para DBFS...\nArquivo copiado: amazon.csv -> /FileStore/tables/data_ingestion/20250707_amazon.csv\n"
     ]
    }
   ],
   "source": [
    "# 8. Copiar arquivos para DBFS com novo nome (apenas data)\n",
    "def copy_to_dbfs(src_path):\n",
    "    # Listar todos os arquivos no diretório de download\n",
    "    for file_name in os.listdir(src_path):\n",
    "        file_path = os.path.join(src_path, file_name)\n",
    "        \n",
    "        if os.path.isfile(file_path):\n",
    "            # Gerar novo nome com apenas a data (sem horário)\n",
    "            date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "            new_file_name = f\"{date_str}_{file_name}\"\n",
    "            dbfs_dest_path = f\"{dbfs_dir}/{new_file_name}\"\n",
    "            \n",
    "            # Copiar arquivo para DBFS\n",
    "            dbutils.fs.cp(f\"file:{file_path}\", dbfs_dest_path)\n",
    "            print(f\"Arquivo copiado: {file_name} -> {dbfs_dest_path}\")\n",
    "            \n",
    "            return dbfs_dest_path\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Copiando arquivos para DBFS...\")\n",
    "final_destination = copy_to_dbfs(download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2c71ba5-3f88-4634-9273-64d0f3c7eb5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n==================================================\nResumo da operação:\nDataset baixado de: kagglehub.dataset_download('karkavelrajaj/amazon-sales-dataset')\nCredenciais carregadas de: /FileStore/tables/kaggle.json\nArquivo final salvo em: /FileStore/tables/data_ingestion/20250707_amazon.csv\n==================================================\n"
     ]
    }
   ],
   "source": [
    "# 9. Resultado final\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Resumo da operação:\")\n",
    "print(f\"Dataset baixado de: kagglehub.dataset_download('karkavelrajaj/amazon-sales-dataset')\")\n",
    "print(f\"Credenciais carregadas de: /FileStore/tables/kaggle.json\")\n",
    "print(f\"Arquivo final salvo em: {final_destination}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ae0dc7-8ef2-4c6e-9184-2d6092723b2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/data_ingestion/20250616_amazon.csv</td><td>20250616_amazon.csv</td><td>4744481</td><td>1750079913000</td></tr><tr><td>dbfs:/FileStore/tables/data_ingestion/20250617_amazon.csv</td><td>20250617_amazon.csv</td><td>4744481</td><td>1750158081000</td></tr><tr><td>dbfs:/FileStore/tables/data_ingestion/20250618_amazon.csv</td><td>20250618_amazon.csv</td><td>4744481</td><td>1750244525000</td></tr><tr><td>dbfs:/FileStore/tables/data_ingestion/20250707_amazon.csv</td><td>20250707_amazon.csv</td><td>4744481</td><td>1751884362000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/data_ingestion/20250616_amazon.csv",
         "20250616_amazon.csv",
         4744481,
         1750079913000
        ],
        [
         "dbfs:/FileStore/tables/data_ingestion/20250617_amazon.csv",
         "20250617_amazon.csv",
         4744481,
         1750158081000
        ],
        [
         "dbfs:/FileStore/tables/data_ingestion/20250618_amazon.csv",
         "20250618_amazon.csv",
         4744481,
         1750244525000
        ],
        [
         "dbfs:/FileStore/tables/data_ingestion/20250707_amazon.csv",
         "20250707_amazon.csv",
         4744481,
         1751884362000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "files = dbutils.fs.ls(\"/FileStore/tables/data_ingestion/\")\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c248e405-85f6-4bdf-817b-6ccfe9577e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+----------------+------------+-------------------+------+------------+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|product_id|        product_name|            category|discounted_price|actual_price|discount_percentage|rating|rating_count|        about_product|             user_id|           user_name|           review_id|        review_title|      review_content|            img_link|        product_link|\n+----------+--------------------+--------------------+----------------+------------+-------------------+------+------------+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|B07JW9H4J1|Wayona Nylon Brai...|Computers&Accesso...|            ₹399|      ₹1,099|                64%|   4.2|      24,269| High Compatibilit...|AG3D6O4STAQKAY2UV...|Manav,Adarsh gupt...|R3HXWT0LRP0NMF,R2...|Satisfied,Chargin...|Looks durable Cha...|https://m.media-a...|https://www.amazo...|\n|B098NS6PVG|Ambrane Unbreakab...|Computers&Accesso...|            ₹199|        ₹349|                43%|   4.0|      43,994| Compatible with a...|AECPFYFQVRUWC3KGN...|ArdKn,Nirbhay kum...|RGIQEG07R9HS2,R1S...|A Good Braided Ca...|I ordered this ca...|https://m.media-a...|https://www.amazo...|\n|B096MSW6CT|Sounce Fast Phone...|Computers&Accesso...|            ₹199|      ₹1,899|                90%|   3.9|       7,928|【 Fast Charger& D...|AGU3BBQ2V2DDAMOAK...|Kunal,Himanshu,vi...|R3J3EQQ9TZI5ZJ,R3...|Good speed for ea...|Not quite durable...|https://m.media-a...|https://www.amazo...|\n|B08HDJ86NZ|boAt Deuce USB 30...|Computers&Accesso...|            ₹329|        ₹699|                53%|   4.2|      94,363| The boAt Deuce US...|AEWAZDZZJLQUYVOVG...|Omkar dhale,JD,HE...|R3EEUZKKK9J36I,R3...|Good product,Good...|Good product,long...|https://m.media-a...|https://www.amazo...|\n|B08CF3B7N1|Portronics Konnec...|Computers&Accesso...|            ₹154|        ₹399|                61%|   4.2|      16,905| [CHARGE & SYNC FU...|AE3Q6KSUK5P75D5HF...|rahuls6099,Swasat...|R1BP4L2HH9TFUP,R1...|As good as origin...|Bought this inste...|https://m.media-a...|https://www.amazo...|\n+----------+--------------------+--------------------+----------------+------------+-------------------+------+------------+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "#teste\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "amazon = spark.read.csv(\"/FileStore/tables/data_ingestion/20250618_amazon.csv\", header=True, sep=\",\",inferSchema=True)\n",
    "amazon.show(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 836700279434220,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "data_Ingestion",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}